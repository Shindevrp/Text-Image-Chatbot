import gradio as gr
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
from PIL import Image
import torch

# Load model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = LlavaNextProcessor.from_pretrained("llava-hf/llava-v1.6-mistral-7b-hf")
model = LlavaNextForConditionalGeneration.from_pretrained(
    "llava-hf/llava-v1.6-mistral-7b-hf",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
)
model.to(device)

def chat_with_image(image, question):
    """
    Processes the uploaded image and user question, and returns a response.
    Note: Large models like LLaVA may take time to respond, especially on CPU. For faster results, use a GPU or reduce max_new_tokens.
    """
    prompt = f"[INST] <image>\n{question} [/INST]"
    inputs = processor(prompt, image, return_tensors="pt").to(device)
    output = model.generate(**inputs, max_new_tokens=30)  # Reduced for faster response
    response = processor.decode(output[0], skip_special_tokens=True)
    return response

iface = gr.Interface(
    fn=chat_with_image,
    inputs=[gr.Image(type="pil"), gr.Textbox(label="Ask a question about the image")],
    outputs="text",
    title="Image-based Chatbot (LLaVA)",
    description=(
        "Upload an image and ask a question. The chatbot will respond based on the image content.\n"
        "\nInstructions:\n"
        "- Please upload clear, relevant images (JPG, PNG, etc.).\n"
        "- Type specific, respectful, and clear questions for best results.\n"
        "- Avoid uploading sensitive, private, or inappropriate images.\n"
        "- The AI is designed to provide helpful, safe, and informative answers.\n"
        "- Responses are generated by an AI model and may not always be perfect—use your own judgment.\n"
        "\nNotes:\n"
        "- This demo uses a large vision-language model and may take time to respond, especially without a GPU.\n"
        "- To share this app, a public link is created.\n"
        "- Informational messages about input order and token settings are normal.\n"
        "- For faster responses, use a smaller model or reduce the question complexity.\n"
        "\n---\n"
        "Made with ❤️ by shinde"
    )
)

if __name__ == "__main__":
    iface.launch(share=True)
